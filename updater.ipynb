{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Updater for BNS Data\n",
    "\n",
    "This notebook updates the ChromaDB vector store with new legal documents from the `data/bns_data` directory.\n",
    "\n",
    "## Files to be processed:\n",
    "- `bns_2024.pdf` - Bharatiya Nyaya Sanhita 2024\n",
    "- `bnss_2024.pdf` - Bharatiya Nagarik Suraksha Sanhita 2024\n",
    "- `bsa_2024.pdf` - Bharatiya Sakshya Adhiniyam 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Paths and Initialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\upama\\AppData\\Local\\Temp\\ipykernel_3820\\2867277281.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\upama\\anaconda3\\envs\\legal-chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model initialized: sentence-transformers/all-MiniLM-L6-v2\n",
      "BNS Data directory: data/bns_data\n",
      "ChromaDB directory: chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "BNS_DATA_DIR = \"data/bns_data\"\n",
    "CHROMA_DIR = \"chroma_db\"\n",
    "\n",
    "# Initialize embeddings model\n",
    "def get_embeddings_model():\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings_model()\n",
    "print(f\"Embeddings model initialized: {embeddings.model_name}\")\n",
    "print(f\"BNS Data directory: {BNS_DATA_DIR}\")\n",
    "print(f\"ChromaDB directory: {CHROMA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Available Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files in data/bns_data:\n",
      "  - bnss_2024.pdf (1.94 MB)\n",
      "  - bns_2024.pdf (1.26 MB)\n",
      "  - bsa_2024.pdf (0.64 MB)\n",
      "  - penal_code_India.pdf (1.05 MB)\n"
     ]
    }
   ],
   "source": [
    "# Check what files are available in bns_data directory\n",
    "if os.path.exists(BNS_DATA_DIR):\n",
    "    bns_files = [f for f in os.listdir(BNS_DATA_DIR) if f.endswith('.pdf')]\n",
    "    print(f\"Found {len(bns_files)} PDF files in {BNS_DATA_DIR}:\")\n",
    "    for file in bns_files:\n",
    "        file_path = os.path.join(BNS_DATA_DIR, file)\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"Directory {BNS_DATA_DIR} not found!\")\n",
    "    bns_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Process BNS Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bnss_2024.pdf...\n",
      "  Loaded 249 pages from bnss_2024.pdf\n",
      "Loading bns_2024.pdf...\n",
      "  Loaded 102 pages from bns_2024.pdf\n",
      "Loading bsa_2024.pdf...\n",
      "  Loaded 47 pages from bsa_2024.pdf\n",
      "Loading penal_code_India.pdf...\n",
      "  Loaded 119 pages from penal_code_India.pdf\n",
      "\n",
      "Total BNS document pages loaded: 517\n"
     ]
    }
   ],
   "source": [
    "def load_bns_documents():\n",
    "    \"\"\"Load PDF documents from BNS data directory\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for file in bns_files:\n",
    "        file_path = os.path.join(BNS_DATA_DIR, file)\n",
    "        try:\n",
    "            print(f\"Loading {file}...\")\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            loaded_docs = loader.load()\n",
    "            \n",
    "            # Add metadata to identify BNS documents\n",
    "            for doc in loaded_docs:\n",
    "                if not doc.metadata:\n",
    "                    doc.metadata = {}\n",
    "                doc.metadata[\"source_type\"] = \"bns_2024\"\n",
    "                doc.metadata[\"priority\"] = \"high\"\n",
    "                doc.metadata[\"document_category\"] = \"new_criminal_laws\"\n",
    "            \n",
    "            documents.extend(loaded_docs)\n",
    "            print(f\"  Loaded {len(loaded_docs)} pages from {file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {file}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load BNS documents\n",
    "bns_documents = load_bns_documents()\n",
    "print(f\"\\nTotal BNS document pages loaded: {len(bns_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 517 documents into 3070 chunks\n",
      "Created 6140 weighted BNS chunks (2x representation)\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into chunks for embedding\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split BNS documents with smaller chunks for better retrieval\n",
    "bns_chunks = split_documents(bns_documents, chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "# Give BNS documents high priority by duplicating chunks (2x representation)\n",
    "weighted_bns_chunks = bns_chunks * 2\n",
    "print(f\"Created {len(weighted_bns_chunks)} weighted BNS chunks (2x representation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Existing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n",
      "Current vector store contains: 21004 documents\n"
     ]
    }
   ],
   "source": [
    "# Load existing vector store\n",
    "if os.path.exists(CHROMA_DIR):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_DIR,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    # Check current collection size\n",
    "    collection = vector_store._collection\n",
    "    current_count = collection.count()\n",
    "    print(f\"Current vector store contains: {current_count} documents\")\n",
    "else:\n",
    "    print(f\"Vector store directory {CHROMA_DIR} not found!\")\n",
    "    print(\"Please run the main ingestion process first.\")\n",
    "    vector_store = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Add BNS Documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding BNS documents to existing vector store...\n",
      "Error adding documents to vector store: ValueError: Batch size of 6140 is greater than max batch size of 5461\n"
     ]
    }
   ],
   "source": [
    "if vector_store and weighted_bns_chunks:\n",
    "    print(\"Adding BNS documents to existing vector store...\")\n",
    "    \n",
    "    try:\n",
    "        # Add the new BNS chunks to existing vector store\n",
    "        vector_store.add_documents(weighted_bns_chunks)\n",
    "        \n",
    "        # Check updated collection size\n",
    "        updated_count = vector_store._collection.count()\n",
    "        added_count = updated_count - current_count\n",
    "        \n",
    "        print(f\"Successfully added {added_count} BNS document chunks!\")\n",
    "        print(f\"Updated vector store now contains: {updated_count} documents\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error adding documents to vector store: {e}\")\n",
    "else:\n",
    "    print(\"Cannot update vector store - either vector store not found or no BNS documents to add.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Retrieval with BNS Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing retrieval with BNS-related queries:\n",
      "\n",
      "Query: What is Bharatiya Nyaya Sanhita?\n",
      "  Retrieved 5 documents\n",
      "  BNS 2024 documents: 0\n",
      "  Other documents: 5\n",
      "\n",
      "Query: BNS 2024 provisions\n",
      "  Retrieved 5 documents\n",
      "  BNS 2024 documents: 0\n",
      "  Other documents: 5\n",
      "\n",
      "Query: Bharatiya Nagarik Suraksha Sanhita\n",
      "  Retrieved 5 documents\n",
      "  BNS 2024 documents: 0\n",
      "  Other documents: 5\n",
      "\n",
      "Query: New criminal laws India 2024\n",
      "  Retrieved 5 documents\n",
      "  BNS 2024 documents: 0\n",
      "  Other documents: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if vector_store:\n",
    "    # Test queries related to new criminal laws\n",
    "    test_queries = [\n",
    "        \"What is Bharatiya Nyaya Sanhita?\",\n",
    "        \"BNS 2024 provisions\",\n",
    "        \"Bharatiya Nagarik Suraksha Sanhita\",\n",
    "        \"New criminal laws India 2024\"\n",
    "    ]\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    print(\"Testing retrieval with BNS-related queries:\\n\")\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        docs = retriever.invoke(query)\n",
    "        \n",
    "        # Count BNS documents in results\n",
    "        bns_count = sum(1 for doc in docs if doc.metadata.get(\"source_type\") == \"bns_2024\")\n",
    "        \n",
    "        print(f\"  Retrieved {len(docs)} documents\")\n",
    "        print(f\"  BNS 2024 documents: {bns_count}\")\n",
    "        print(f\"  Other documents: {len(docs) - bns_count}\")\n",
    "        \n",
    "        if docs and docs[0].metadata.get(\"source_type\") == \"bns_2024\":\n",
    "            print(f\"  ✓ BNS document found in top result\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Cannot test retrieval - vector store not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary and Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATABASE UPDATE SUMMARY\n",
      "============================================================\n",
      "✓ Processed 4 BNS PDF files:\n",
      "  - bnss_2024.pdf\n",
      "  - bns_2024.pdf\n",
      "  - bsa_2024.pdf\n",
      "  - penal_code_India.pdf\n",
      "\n",
      "✓ Created 3070 document chunks\n",
      "✓ Applied 2x weighting: 6140 total chunks\n",
      "✓ Successfully updated ChromaDB vector store\n",
      "✓ Database now contains enhanced BNS 2024 content\n",
      "\n",
      "🎉 Update completed successfully!\n",
      "\n",
      "The legal assistant chatbot now has access to:\n",
      "  • Bharatiya Nyaya Sanhita 2024\n",
      "  • Bharatiya Nagarik Suraksha Sanhita 2024\n",
      "  • Bharatiya Sakshya Adhiniyam 2024\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATABASE UPDATE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if bns_files:\n",
    "    print(f\"✓ Processed {len(bns_files)} BNS PDF files:\")\n",
    "    for file in bns_files:\n",
    "        print(f\"  - {file}\")\n",
    "    \n",
    "    print(f\"\\n✓ Created {len(bns_chunks)} document chunks\")\n",
    "    print(f\"✓ Applied 2x weighting: {len(weighted_bns_chunks)} total chunks\")\n",
    "    \n",
    "    if vector_store:\n",
    "        print(f\"✓ Successfully updated ChromaDB vector store\")\n",
    "        print(f\"✓ Database now contains enhanced BNS 2024 content\")\n",
    "        print(\"\\n🎉 Update completed successfully!\")\n",
    "        print(\"\\nThe legal assistant chatbot now has access to:\")\n",
    "        print(\"  • Bharatiya Nyaya Sanhita 2024\")\n",
    "        print(\"  • Bharatiya Nagarik Suraksha Sanhita 2024\")\n",
    "        print(\"  • Bharatiya Sakshya Adhiniyam 2024\")\n",
    "    else:\n",
    "        print(\"⚠️  Vector store not found - please run main ingestion first\")\n",
    "else:\n",
    "    print(\"⚠️  No BNS files found in data/bns_data directory\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
